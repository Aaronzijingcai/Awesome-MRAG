INFO 09-10 10:19:59 [__init__.py:235] Automatically detected platform cuda.
INFO 09-10 10:20:01 [api_server.py:1755] vLLM API server version 0.10.0
INFO 09-10 10:20:01 [cli_args.py:261] non-default args: {'host': '0.0.0.0', 'port': 8888, 'model': '/NAS/caizj/models/deepseek/DeepSeek-R1-Distill-Qwen-1.5B/'}
INFO 09-10 10:20:07 [config.py:1604] Using max model len 131072
INFO 09-10 10:20:08 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 09-10 10:20:12 [__init__.py:235] Automatically detected platform cuda.
INFO 09-10 10:20:14 [core.py:572] Waiting for init message from front-end.
INFO 09-10 10:20:14 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='/NAS/caizj/models/deepseek/DeepSeek-R1-Distill-Qwen-1.5B/', speculative_config=None, tokenizer='/NAS/caizj/models/deepseek/DeepSeek-R1-Distill-Qwen-1.5B/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/NAS/caizj/models/deepseek/DeepSeek-R1-Distill-Qwen-1.5B/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 09-10 10:20:16 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 09-10 10:20:16 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 09-10 10:20:16 [gpu_model_runner.py:1843] Starting to load model /NAS/caizj/models/deepseek/DeepSeek-R1-Distill-Qwen-1.5B/...
INFO 09-10 10:20:16 [gpu_model_runner.py:1875] Loading model from scratch...
INFO 09-10 10:20:16 [cuda.py:290] Using Flash Attention backend on V1 engine.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.75it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.75it/s]

INFO 09-10 10:20:17 [default_loader.py:262] Loading weights took 0.73 seconds
INFO 09-10 10:20:17 [gpu_model_runner.py:1892] Model loading took 3.3466 GiB and 0.852506 seconds
INFO 09-10 10:20:23 [backends.py:530] Using cache directory: /home/caizj/.cache/vllm/torch_compile_cache/27930d4337/rank_0_0/backbone for vLLM's torch.compile
INFO 09-10 10:20:23 [backends.py:541] Dynamo bytecode transform time: 5.73 s
INFO 09-10 10:20:27 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.591 s
INFO 09-10 10:20:27 [monitor.py:34] torch.compile takes 5.73 s in total
INFO 09-10 10:20:28 [gpu_worker.py:255] Available KV cache memory: 16.56 GiB
INFO 09-10 10:20:28 [kv_cache_utils.py:833] GPU KV cache size: 620,192 tokens
INFO 09-10 10:20:28 [kv_cache_utils.py:837] Maximum concurrency for 131,072 tokens per request: 4.73x
Capturing CUDA graph shapes:   0%|          | 0/67 [00:00<?, ?it/s]Capturing CUDA graph shapes:   4%|▍         | 3/67 [00:00<00:02, 27.80it/s]Capturing CUDA graph shapes:   9%|▉         | 6/67 [00:00<00:02, 28.26it/s]Capturing CUDA graph shapes:  13%|█▎        | 9/67 [00:00<00:02, 27.13it/s]Capturing CUDA graph shapes:  18%|█▊        | 12/67 [00:00<00:02, 27.02it/s]Capturing CUDA graph shapes:  22%|██▏       | 15/67 [00:00<00:01, 26.69it/s]Capturing CUDA graph shapes:  27%|██▋       | 18/67 [00:00<00:01, 27.69it/s]Capturing CUDA graph shapes:  33%|███▎      | 22/67 [00:00<00:01, 29.21it/s]Capturing CUDA graph shapes:  39%|███▉      | 26/67 [00:00<00:01, 30.18it/s]Capturing CUDA graph shapes:  45%|████▍     | 30/67 [00:01<00:01, 30.67it/s]Capturing CUDA graph shapes:  51%|█████     | 34/67 [00:01<00:01, 31.50it/s]Capturing CUDA graph shapes:  57%|█████▋    | 38/67 [00:01<00:00, 32.49it/s]Capturing CUDA graph shapes:  63%|██████▎   | 42/67 [00:01<00:00, 33.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 46/67 [00:01<00:00, 35.22it/s]Capturing CUDA graph shapes:  76%|███████▌  | 51/67 [00:01<00:00, 37.67it/s]Capturing CUDA graph shapes:  84%|████████▎ | 56/67 [00:01<00:00, 39.67it/s]Capturing CUDA graph shapes:  91%|█████████ | 61/67 [00:01<00:00, 40.07it/s]Capturing CUDA graph shapes:  99%|█████████▊| 66/67 [00:01<00:00, 41.82it/s]Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:01<00:00, 33.96it/s]
INFO 09-10 10:20:31 [gpu_model_runner.py:2485] Graph capturing finished in 2 secs, took 0.46 GiB
INFO 09-10 10:20:31 [core.py:193] init engine (profile, create kv cache, warmup model) took 13.58 seconds
INFO 09-10 10:20:31 [loggers.py:141] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 38762
WARNING 09-10 10:20:31 [config.py:1528] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
INFO 09-10 10:20:31 [serving_responses.py:89] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.95}
INFO 09-10 10:20:31 [serving_chat.py:122] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.95}
INFO 09-10 10:20:31 [serving_completion.py:77] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.95}
INFO 09-10 10:20:31 [api_server.py:1818] Starting vLLM API server 0 on http://0.0.0.0:8888
INFO 09-10 10:20:31 [launcher.py:29] Available routes are:
INFO 09-10 10:20:31 [launcher.py:37] Route: /openapi.json, Methods: HEAD, GET
INFO 09-10 10:20:31 [launcher.py:37] Route: /docs, Methods: HEAD, GET
INFO 09-10 10:20:31 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 09-10 10:20:31 [launcher.py:37] Route: /redoc, Methods: HEAD, GET
INFO 09-10 10:20:31 [launcher.py:37] Route: /health, Methods: GET
INFO 09-10 10:20:31 [launcher.py:37] Route: /load, Methods: GET
INFO 09-10 10:20:31 [launcher.py:37] Route: /ping, Methods: POST
INFO 09-10 10:20:31 [launcher.py:37] Route: /ping, Methods: GET
INFO 09-10 10:20:31 [launcher.py:37] Route: /tokenize, Methods: POST
INFO 09-10 10:20:31 [launcher.py:37] Route: /detokenize, Methods: POST
INFO 09-10 10:20:31 [launcher.py:37] Route: /v1/models, Methods: GET
INFO 09-10 10:20:31 [launcher.py:37] Route: /version, Methods: GET
INFO 09-10 10:20:31 [launcher.py:37] Route: /v1/responses, Methods: POST
INFO 09-10 10:20:31 [launcher.py:37] Route: /v1/responses/{response_id}, Methods: GET
INFO 09-10 10:20:31 [launcher.py:37] Route: /v1/responses/{response_id}/cancel, Methods: POST
INFO 09-10 10:20:31 [launcher.py:37] Route: /v1/chat/completions, Methods: POST
INFO 09-10 10:20:31 [launcher.py:37] Route: /v1/completions, Methods: POST
INFO 09-10 10:20:31 [launcher.py:37] Route: /v1/embeddings, Methods: POST
INFO 09-10 10:20:31 [launcher.py:37] Route: /pooling, Methods: POST
INFO 09-10 10:20:31 [launcher.py:37] Route: /classify, Methods: POST
INFO 09-10 10:20:31 [launcher.py:37] Route: /score, Methods: POST
INFO 09-10 10:20:31 [launcher.py:37] Route: /v1/score, Methods: POST
INFO 09-10 10:20:31 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST
INFO 09-10 10:20:31 [launcher.py:37] Route: /v1/audio/translations, Methods: POST
INFO 09-10 10:20:31 [launcher.py:37] Route: /rerank, Methods: POST
INFO 09-10 10:20:31 [launcher.py:37] Route: /v1/rerank, Methods: POST
INFO 09-10 10:20:31 [launcher.py:37] Route: /v2/rerank, Methods: POST
INFO 09-10 10:20:31 [launcher.py:37] Route: /scale_elastic_ep, Methods: POST
INFO 09-10 10:20:31 [launcher.py:37] Route: /is_scaling_elastic_ep, Methods: POST
INFO 09-10 10:20:31 [launcher.py:37] Route: /invocations, Methods: POST
INFO 09-10 10:20:31 [launcher.py:37] Route: /metrics, Methods: GET
INFO:     Started server process [1204700]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO 09-10 22:50:56 [chat_utils.py:473] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
INFO 09-10 22:50:56 [logger.py:41] Received request chatcmpl-bcff1b3b453f4da198097891c4673202: prompt: "<｜begin▁of▁sentence｜><｜User｜>You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: Instruct: Given a search query, retrieve relevant passages that answer the query\nQuery: 设备出现故障该如何解决？ \nContext: [174] Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo,\nand Kaifu Zhang. Marco-o1: Towards open reasoning models for open-ended solutions, 2024. URL\nhttps://arxiv.org/abs/2411.14405.\n67\n\nChen, Yankai Lin, Jie Xie, Wei Zhou, Wang Xu, Yuanheng Zhang, Zhou Su, Zhongwu Zhai, Xiaoming\nLiu, Yudong Mei, Jianming Xu, Hongyan Tian, Chongyi Wang, Chi Chen, Yuan Yao, Zhiyuan Liu, and\nMaosong Sun. Agentcpm-gui: Building mobile-use agents with reinforcement fine-tuning, 2025. URL\nhttps://arxiv.org/abs/2506.01391.\n82\n\n[448] Lei Chen, Xuanle Zhao, Zhixiong Zeng, Jing Huang, Liming Zheng, Yufeng Zhong, and Lin Ma.\nBreaking the sft plateau: Multimodal structured reinforcement learning for chart-to-code generation.\narXiv preprint arXiv:2508.13587, 2025. URLhttps://arxiv.org/abs/2508.13587.\n90\n\n2025. URLhttps://arxiv.org/abs/2505.21668.\n[289] Jie Wu, Haoling Li, Xin Zhang, Jianwen Luo, Yangyu Huang, Ruihang Chu, Yujiu Yang, and Scarlett\nLi. Iterpref: Focal preference learning for code generation via iterative debugging, 2025. URL\nhttps://arxiv.org/abs/2503.02783.\n[290] Nan Jiang, Xiaopeng Li, Shiqi Wang, Qiang Zhou, Soneya Binta Hossain, Baishakhi Ray, Varun\nKumar, Xiaofei Ma, and Anoop Deoras. Ledex: Training llms to better self-debug and explain code.\nIn A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors,\nAdvances in Neural Information Processing Systems, volume 37, pages 35517–35543. Curran Associates,\nInc., 2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/file/\n3ea832724870c700f0a03c665572e2a9-Paper-Conference.pdf.\n[291] Zhihui Xie, Jie chen, Liyu Chen, Weichao Mao, Jingjing Xu, and Lingpeng Kong. Teaching language\nmodels to critique via reinforcement learning. InForty-second International Conference on Machine\n\nstrategic flexibility to adapt to novel scenarios or recover from unforeseen errors, a limitation that RL-centric\napproaches directly address by shifting the learning objective from imitation to outcome-driven optimization.\nTool-integrated RL. Building on the limitations of purely imitative paradigms, RL-based approaches\nfor tool use shift the objective from replicating fixed patterns to optimizing end-task performance. This\ntransition enables agents tostrategically decide when, how, andin what combinationto invoke tools, adapting\ndynamically to novel contexts and unforeseen failures. At the foundation, frameworks such as ToolRL [83]\ndemonstrate that, even when initialized from base models without any imitation traces, RL training can\nelicit emergent capabilities, e.g., self-correction of faulty code, adaptive adjustment of invocation frequency,\nand the composition of multiple tools for complex sub-tasks. Subsequently, a recent surge in research \nAnswer:<｜Assistant｜><think>\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=130123, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None.
INFO 09-10 22:50:56 [async_llm.py:269] Added request chatcmpl-bcff1b3b453f4da198097891c4673202.
INFO:     127.0.0.1:51732 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 09-10 22:51:03 [loggers.py:122] Engine 000: Avg prompt throughput: 94.9 tokens/s, Avg generation throughput: 34.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 09-10 22:51:13 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 09-10 22:52:45 [logger.py:41] Received request chatcmpl-d27c5cc2fa004baeb568f70bddb7a714: prompt: "<｜begin▁of▁sentence｜><｜User｜>You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: Instruct: Given a search query, retrieve relevant passages that answer the query\nQuery: Agent的定义是什么？ \nContext: Chen, Yankai Lin, Jie Xie, Wei Zhou, Wang Xu, Yuanheng Zhang, Zhou Su, Zhongwu Zhai, Xiaoming\nLiu, Yudong Mei, Jianming Xu, Hongyan Tian, Chongyi Wang, Chi Chen, Yuan Yao, Zhiyuan Liu, and\nMaosong Sun. Agentcpm-gui: Building mobile-use agents with reinforcement fine-tuning, 2025. URL\nhttps://arxiv.org/abs/2506.01391.\n82\n\nbenchmark for browsing agents, 2025. URLhttps://arxiv.org/abs/2504.12516.\n[267] Yihong Dong, Xue Jiang, Jiaru Qian, Tian Wang, Kechi Zhang, Zhi Jin, and Ge Li. A survey on code\ngeneration with llm-based agents, 2025. URLhttps://arxiv.org/abs/2508.00083.\n[268] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao\nZhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao,\nChenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for a multi-agent collaborative\nframework. InThe Twelfth International Conference on Learning Representations, 2024. URLhttps:\n//openreview.net/forum?id=VtmBAGCN7o.\n[269] Significant Gravitas. AutoGPT: Autonomous gpt-4 agent framework. GitHub, MIT License, 3 2023.\nURL https://github.com/Significant-Gravitas/AutoGPT. Initial release date.\n[270] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi\n\nInfiGUI\nAgent\nUI-\nAGILE\nLeanabell-\nPro ver\nDeepSeek-\nPro ver-v2\nLeanabell-\nPro ver-v2\nSTP\nLeanPr ogress\nKim ina-\nPro ver\nInte rnLM 2. 5-\nSte pProve r\nLean-\nSTaR\nInFiGUI-R1\nAgentCPM\nUI-R1\nZeroGUI\nWebAgent-R1 UI-Venus\nDiGiRL\nUI-TARS\nMaAS\nMAPoRL\nMALT\nReMA\nLERO\nMARFT\nMMedAgent-RL MLPO\nFlow\nReasoner\nTime-R1\nG-Desig\n-ner\nGPT\nSwarm\nTimeMaster\nL-Zero\nAgent\nMod els\nARIA\nGiGPO\nSPA-RL\nTrinity-\nRFT\nSotopia\nRL\nAMPO\nSKyRL\nSQL\nWeak4\nStro ng\nAgent\nPrune\nAgent\nDr opou t\nSirius\nSirius\nACC-\nCollab\nThe Evolution Tree of RL for \nDomain-specific Agents\nMinimo\nPro ofN et++\nCh ain-of-\nA gen ts\nASearc her\nAtom-Sear cher\nMiro Mind\nOS-R1 Re:Form\nMSRL\nSeed-\nProver\nrStar2-\nAgen t\nComputerRL\nMAGRPO\nRL CC F\nFigure 6: The evolution tree of RL for domain-specific agents.\n4.1. Search & Research Agent\nSearch has been central to extending LLMs with external knowledge, with Retrieval-Augmented Generation\n(RAG) as a widely used approach [244, 245]. The paradigm is now evolving beyond simple information\n\nand Feng Zhao. Agent-FLAN: Designing data and methods of effective agent tuning for large\nlanguage models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors,Findings of the\nAssociation for Computational Linguistics: ACL 2024, pages 9354–9366, Bangkok, Thailand, August\n2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.557. URL\nhttps://aclanthology.org/2024.findings-acl.557/.\n[81] Yifan Song, Weimin Xiong, Xiutian Zhao, Dawei Zhu, Wenhao Wu, Ke Wang, Cheng Li, Wei Peng,\nand Sujian Li. Agentbank: Towards generalized llm agents via fine-tuning on 50000+ interaction\ntrajectories. InEMNLP (Findings), pages 2124–2141, 2024. URLhttps://aclanthology.org/\n2024.findings-emnlp.116.\n[82] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang,\nand Yongbin Li. Api-bank: A comprehensive benchmark for tool-augmented llms. InEMNLP, pages\n3102–3116, 2023. URLhttps://doi.org/10.18653/v1/2023.emnlp-main.187.\n\nKeywords: Agentic Reinforcement Learning, Large Language Models, LLM Agent\nDate: September 1st, 2025\nCode Repository: https://github.com/xhyumiracle/Awesome-AgenticLLM-RL-Papers\nCorresponding: jeremyyin@robots.ox.ac.uk, bailei@pjlab.org.cn\nMain Contact: guibinz@u.nus.edu, genghejia0530@gmail.com, x.yu21@imperial.ac.uk\n1\narXiv:2509.02547v1  [cs.AI]  2 Sep 2025 \nAnswer:<｜Assistant｜><think>\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=129648, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None.
INFO 09-10 22:52:45 [async_llm.py:269] Added request chatcmpl-d27c5cc2fa004baeb568f70bddb7a714.
INFO:     127.0.0.1:39540 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 09-10 22:52:53 [loggers.py:122] Engine 000: Avg prompt throughput: 142.4 tokens/s, Avg generation throughput: 25.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 2.7%
INFO 09-10 22:53:03 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 2.7%
INFO 09-10 22:57:19 [logger.py:41] Received request chatcmpl-24e000bb2b044f968aa0951ea8e1b046: prompt: "<｜begin▁of▁sentence｜><｜User｜>You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: Instruct: Given a search query, retrieve relevant passages that answer the query\nQuery: Agent的定义是什么？ \nContext: Chen, Yankai Lin, Jie Xie, Wei Zhou, Wang Xu, Yuanheng Zhang, Zhou Su, Zhongwu Zhai, Xiaoming\nLiu, Yudong Mei, Jianming Xu, Hongyan Tian, Chongyi Wang, Chi Chen, Yuan Yao, Zhiyuan Liu, and\nMaosong Sun. Agentcpm-gui: Building mobile-use agents with reinforcement fine-tuning, 2025. URL\nhttps://arxiv.org/abs/2506.01391.\n82\n\nbenchmark for browsing agents, 2025. URLhttps://arxiv.org/abs/2504.12516.\n[267] Yihong Dong, Xue Jiang, Jiaru Qian, Tian Wang, Kechi Zhang, Zhi Jin, and Ge Li. A survey on code\ngeneration with llm-based agents, 2025. URLhttps://arxiv.org/abs/2508.00083.\n[268] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao\nZhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao,\nChenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for a multi-agent collaborative\nframework. InThe Twelfth International Conference on Learning Representations, 2024. URLhttps:\n//openreview.net/forum?id=VtmBAGCN7o.\n[269] Significant Gravitas. AutoGPT: Autonomous gpt-4 agent framework. GitHub, MIT License, 3 2023.\nURL https://github.com/Significant-Gravitas/AutoGPT. Initial release date.\n[270] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi\n\nInfiGUI\nAgent\nUI-\nAGILE\nLeanabell-\nPro ver\nDeepSeek-\nPro ver-v2\nLeanabell-\nPro ver-v2\nSTP\nLeanPr ogress\nKim ina-\nPro ver\nInte rnLM 2. 5-\nSte pProve r\nLean-\nSTaR\nInFiGUI-R1\nAgentCPM\nUI-R1\nZeroGUI\nWebAgent-R1 UI-Venus\nDiGiRL\nUI-TARS\nMaAS\nMAPoRL\nMALT\nReMA\nLERO\nMARFT\nMMedAgent-RL MLPO\nFlow\nReasoner\nTime-R1\nG-Desig\n-ner\nGPT\nSwarm\nTimeMaster\nL-Zero\nAgent\nMod els\nARIA\nGiGPO\nSPA-RL\nTrinity-\nRFT\nSotopia\nRL\nAMPO\nSKyRL\nSQL\nWeak4\nStro ng\nAgent\nPrune\nAgent\nDr opou t\nSirius\nSirius\nACC-\nCollab\nThe Evolution Tree of RL for \nDomain-specific Agents\nMinimo\nPro ofN et++\nCh ain-of-\nA gen ts\nASearc her\nAtom-Sear cher\nMiro Mind\nOS-R1 Re:Form\nMSRL\nSeed-\nProver\nrStar2-\nAgen t\nComputerRL\nMAGRPO\nRL CC F\nFigure 6: The evolution tree of RL for domain-specific agents.\n4.1. Search & Research Agent\nSearch has been central to extending LLMs with external knowledge, with Retrieval-Augmented Generation\n(RAG) as a widely used approach [244, 245]. The paradigm is now evolving beyond simple information\n\nand Feng Zhao. Agent-FLAN: Designing data and methods of effective agent tuning for large\nlanguage models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors,Findings of the\nAssociation for Computational Linguistics: ACL 2024, pages 9354–9366, Bangkok, Thailand, August\n2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.557. URL\nhttps://aclanthology.org/2024.findings-acl.557/.\n[81] Yifan Song, Weimin Xiong, Xiutian Zhao, Dawei Zhu, Wenhao Wu, Ke Wang, Cheng Li, Wei Peng,\nand Sujian Li. Agentbank: Towards generalized llm agents via fine-tuning on 50000+ interaction\ntrajectories. InEMNLP (Findings), pages 2124–2141, 2024. URLhttps://aclanthology.org/\n2024.findings-emnlp.116.\n[82] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang,\nand Yongbin Li. Api-bank: A comprehensive benchmark for tool-augmented llms. InEMNLP, pages\n3102–3116, 2023. URLhttps://doi.org/10.18653/v1/2023.emnlp-main.187.\n\nKeywords: Agentic Reinforcement Learning, Large Language Models, LLM Agent\nDate: September 1st, 2025\nCode Repository: https://github.com/xhyumiracle/Awesome-AgenticLLM-RL-Papers\nCorresponding: jeremyyin@robots.ox.ac.uk, bailei@pjlab.org.cn\nMain Contact: guibinz@u.nus.edu, genghejia0530@gmail.com, x.yu21@imperial.ac.uk\n1\narXiv:2509.02547v1  [cs.AI]  2 Sep 2025 \nAnswer:<｜Assistant｜><think>\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=129648, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None.
INFO 09-10 22:57:19 [async_llm.py:269] Added request chatcmpl-24e000bb2b044f968aa0951ea8e1b046.
INFO 09-10 22:57:23 [loggers.py:122] Engine 000: Avg prompt throughput: 142.4 tokens/s, Avg generation throughput: 65.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 38.8%
INFO:     127.0.0.1:42998 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 09-10 22:57:33 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 3.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 38.8%
INFO 09-10 22:57:43 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 38.8%
INFO 09-10 23:03:37 [logger.py:41] Received request chatcmpl-115d7e553124497e8ff881e68fa9da14: prompt: "<｜begin▁of▁sentence｜><｜User｜>You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: Instruct: Given a search query, retrieve relevant passages that answer the query\nQuery: Agent的定义是什么？ \nContext: Chen, Yankai Lin, Jie Xie, Wei Zhou, Wang Xu, Yuanheng Zhang, Zhou Su, Zhongwu Zhai, Xiaoming\nLiu, Yudong Mei, Jianming Xu, Hongyan Tian, Chongyi Wang, Chi Chen, Yuan Yao, Zhiyuan Liu, and\nMaosong Sun. Agentcpm-gui: Building mobile-use agents with reinforcement fine-tuning, 2025. URL\nhttps://arxiv.org/abs/2506.01391.\n82\n\nbenchmark for browsing agents, 2025. URLhttps://arxiv.org/abs/2504.12516.\n[267] Yihong Dong, Xue Jiang, Jiaru Qian, Tian Wang, Kechi Zhang, Zhi Jin, and Ge Li. A survey on code\ngeneration with llm-based agents, 2025. URLhttps://arxiv.org/abs/2508.00083.\n[268] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao\nZhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao,\nChenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for a multi-agent collaborative\nframework. InThe Twelfth International Conference on Learning Representations, 2024. URLhttps:\n//openreview.net/forum?id=VtmBAGCN7o.\n[269] Significant Gravitas. AutoGPT: Autonomous gpt-4 agent framework. GitHub, MIT License, 3 2023.\nURL https://github.com/Significant-Gravitas/AutoGPT. Initial release date.\n[270] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi\n\nInfiGUI\nAgent\nUI-\nAGILE\nLeanabell-\nPro ver\nDeepSeek-\nPro ver-v2\nLeanabell-\nPro ver-v2\nSTP\nLeanPr ogress\nKim ina-\nPro ver\nInte rnLM 2. 5-\nSte pProve r\nLean-\nSTaR\nInFiGUI-R1\nAgentCPM\nUI-R1\nZeroGUI\nWebAgent-R1 UI-Venus\nDiGiRL\nUI-TARS\nMaAS\nMAPoRL\nMALT\nReMA\nLERO\nMARFT\nMMedAgent-RL MLPO\nFlow\nReasoner\nTime-R1\nG-Desig\n-ner\nGPT\nSwarm\nTimeMaster\nL-Zero\nAgent\nMod els\nARIA\nGiGPO\nSPA-RL\nTrinity-\nRFT\nSotopia\nRL\nAMPO\nSKyRL\nSQL\nWeak4\nStro ng\nAgent\nPrune\nAgent\nDr opou t\nSirius\nSirius\nACC-\nCollab\nThe Evolution Tree of RL for \nDomain-specific Agents\nMinimo\nPro ofN et++\nCh ain-of-\nA gen ts\nASearc her\nAtom-Sear cher\nMiro Mind\nOS-R1 Re:Form\nMSRL\nSeed-\nProver\nrStar2-\nAgen t\nComputerRL\nMAGRPO\nRL CC F\nFigure 6: The evolution tree of RL for domain-specific agents.\n4.1. Search & Research Agent\nSearch has been central to extending LLMs with external knowledge, with Retrieval-Augmented Generation\n(RAG) as a widely used approach [244, 245]. The paradigm is now evolving beyond simple information\n\nand Feng Zhao. Agent-FLAN: Designing data and methods of effective agent tuning for large\nlanguage models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors,Findings of the\nAssociation for Computational Linguistics: ACL 2024, pages 9354–9366, Bangkok, Thailand, August\n2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.557. URL\nhttps://aclanthology.org/2024.findings-acl.557/.\n[81] Yifan Song, Weimin Xiong, Xiutian Zhao, Dawei Zhu, Wenhao Wu, Ke Wang, Cheng Li, Wei Peng,\nand Sujian Li. Agentbank: Towards generalized llm agents via fine-tuning on 50000+ interaction\ntrajectories. InEMNLP (Findings), pages 2124–2141, 2024. URLhttps://aclanthology.org/\n2024.findings-emnlp.116.\n[82] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang,\nand Yongbin Li. Api-bank: A comprehensive benchmark for tool-augmented llms. InEMNLP, pages\n3102–3116, 2023. URLhttps://doi.org/10.18653/v1/2023.emnlp-main.187.\n\nKeywords: Agentic Reinforcement Learning, Large Language Models, LLM Agent\nDate: September 1st, 2025\nCode Repository: https://github.com/xhyumiracle/Awesome-AgenticLLM-RL-Papers\nCorresponding: jeremyyin@robots.ox.ac.uk, bailei@pjlab.org.cn\nMain Contact: guibinz@u.nus.edu, genghejia0530@gmail.com, x.yu21@imperial.ac.uk\n1\narXiv:2509.02547v1  [cs.AI]  2 Sep 2025 \nAnswer:<｜Assistant｜><think>\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=129648, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None.
INFO 09-10 23:03:37 [async_llm.py:269] Added request chatcmpl-115d7e553124497e8ff881e68fa9da14.
INFO:     127.0.0.1:46932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 09-10 23:03:43 [loggers.py:122] Engine 000: Avg prompt throughput: 142.4 tokens/s, Avg generation throughput: 26.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 55.2%
INFO 09-10 23:03:53 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 55.2%
INFO 09-10 23:22:47 [logger.py:41] Received request chatcmpl-eb7daf25495b4744a36548678159422d: prompt: "<｜begin▁of▁sentence｜><｜User｜>You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: Instruct: Given a search query, retrieve relevant passages that answer the query\nQuery: Agent的定义是什么？ \nContext: Chen, Yankai Lin, Jie Xie, Wei Zhou, Wang Xu, Yuanheng Zhang, Zhou Su, Zhongwu Zhai, Xiaoming\nLiu, Yudong Mei, Jianming Xu, Hongyan Tian, Chongyi Wang, Chi Chen, Yuan Yao, Zhiyuan Liu, and\nMaosong Sun. Agentcpm-gui: Building mobile-use agents with reinforcement fine-tuning, 2025. URL\nhttps://arxiv.org/abs/2506.01391.\n82\n\nbenchmark for browsing agents, 2025. URLhttps://arxiv.org/abs/2504.12516.\n[267] Yihong Dong, Xue Jiang, Jiaru Qian, Tian Wang, Kechi Zhang, Zhi Jin, and Ge Li. A survey on code\ngeneration with llm-based agents, 2025. URLhttps://arxiv.org/abs/2508.00083.\n[268] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao\nZhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao,\nChenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for a multi-agent collaborative\nframework. InThe Twelfth International Conference on Learning Representations, 2024. URLhttps:\n//openreview.net/forum?id=VtmBAGCN7o.\n[269] Significant Gravitas. AutoGPT: Autonomous gpt-4 agent framework. GitHub, MIT License, 3 2023.\nURL https://github.com/Significant-Gravitas/AutoGPT. Initial release date.\n[270] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi\n\nInfiGUI\nAgent\nUI-\nAGILE\nLeanabell-\nPro ver\nDeepSeek-\nPro ver-v2\nLeanabell-\nPro ver-v2\nSTP\nLeanPr ogress\nKim ina-\nPro ver\nInte rnLM 2. 5-\nSte pProve r\nLean-\nSTaR\nInFiGUI-R1\nAgentCPM\nUI-R1\nZeroGUI\nWebAgent-R1 UI-Venus\nDiGiRL\nUI-TARS\nMaAS\nMAPoRL\nMALT\nReMA\nLERO\nMARFT\nMMedAgent-RL MLPO\nFlow\nReasoner\nTime-R1\nG-Desig\n-ner\nGPT\nSwarm\nTimeMaster\nL-Zero\nAgent\nMod els\nARIA\nGiGPO\nSPA-RL\nTrinity-\nRFT\nSotopia\nRL\nAMPO\nSKyRL\nSQL\nWeak4\nStro ng\nAgent\nPrune\nAgent\nDr opou t\nSirius\nSirius\nACC-\nCollab\nThe Evolution Tree of RL for \nDomain-specific Agents\nMinimo\nPro ofN et++\nCh ain-of-\nA gen ts\nASearc her\nAtom-Sear cher\nMiro Mind\nOS-R1 Re:Form\nMSRL\nSeed-\nProver\nrStar2-\nAgen t\nComputerRL\nMAGRPO\nRL CC F\nFigure 6: The evolution tree of RL for domain-specific agents.\n4.1. Search & Research Agent\nSearch has been central to extending LLMs with external knowledge, with Retrieval-Augmented Generation\n(RAG) as a widely used approach [244, 245]. The paradigm is now evolving beyond simple information\n\nand Feng Zhao. Agent-FLAN: Designing data and methods of effective agent tuning for large\nlanguage models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors,Findings of the\nAssociation for Computational Linguistics: ACL 2024, pages 9354–9366, Bangkok, Thailand, August\n2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.557. URL\nhttps://aclanthology.org/2024.findings-acl.557/.\n[81] Yifan Song, Weimin Xiong, Xiutian Zhao, Dawei Zhu, Wenhao Wu, Ke Wang, Cheng Li, Wei Peng,\nand Sujian Li. Agentbank: Towards generalized llm agents via fine-tuning on 50000+ interaction\ntrajectories. InEMNLP (Findings), pages 2124–2141, 2024. URLhttps://aclanthology.org/\n2024.findings-emnlp.116.\n[82] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang,\nand Yongbin Li. Api-bank: A comprehensive benchmark for tool-augmented llms. InEMNLP, pages\n3102–3116, 2023. URLhttps://doi.org/10.18653/v1/2023.emnlp-main.187.\n\nKeywords: Agentic Reinforcement Learning, Large Language Models, LLM Agent\nDate: September 1st, 2025\nCode Repository: https://github.com/xhyumiracle/Awesome-AgenticLLM-RL-Papers\nCorresponding: jeremyyin@robots.ox.ac.uk, bailei@pjlab.org.cn\nMain Contact: guibinz@u.nus.edu, genghejia0530@gmail.com, x.yu21@imperial.ac.uk\n1\narXiv:2509.02547v1  [cs.AI]  2 Sep 2025 \nAnswer:<｜Assistant｜><think>\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=129648, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None.
INFO 09-10 23:22:47 [async_llm.py:269] Added request chatcmpl-eb7daf25495b4744a36548678159422d.
INFO:     127.0.0.1:33560 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 09-10 23:22:53 [loggers.py:122] Engine 000: Avg prompt throughput: 142.4 tokens/s, Avg generation throughput: 40.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 64.5%
INFO 09-10 23:23:03 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 64.5%
INFO 09-10 23:26:21 [logger.py:41] Received request chatcmpl-56ffd8b332e54aa49b2c8233721c9d51: prompt: "<｜begin▁of▁sentence｜><｜User｜>You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: Instruct: Given a search query, retrieve relevant passages that answer the query\nQuery: Agent的定义是什么？ \nContext: Chen, Yankai Lin, Jie Xie, Wei Zhou, Wang Xu, Yuanheng Zhang, Zhou Su, Zhongwu Zhai, Xiaoming\nLiu, Yudong Mei, Jianming Xu, Hongyan Tian, Chongyi Wang, Chi Chen, Yuan Yao, Zhiyuan Liu, and\nMaosong Sun. Agentcpm-gui: Building mobile-use agents with reinforcement fine-tuning, 2025. URL\nhttps://arxiv.org/abs/2506.01391.\n82\n\nbenchmark for browsing agents, 2025. URLhttps://arxiv.org/abs/2504.12516.\n[267] Yihong Dong, Xue Jiang, Jiaru Qian, Tian Wang, Kechi Zhang, Zhi Jin, and Ge Li. A survey on code\ngeneration with llm-based agents, 2025. URLhttps://arxiv.org/abs/2508.00083.\n[268] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao\nZhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao,\nChenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for a multi-agent collaborative\nframework. InThe Twelfth International Conference on Learning Representations, 2024. URLhttps:\n//openreview.net/forum?id=VtmBAGCN7o.\n[269] Significant Gravitas. AutoGPT: Autonomous gpt-4 agent framework. GitHub, MIT License, 3 2023.\nURL https://github.com/Significant-Gravitas/AutoGPT. Initial release date.\n[270] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi\n\nInfiGUI\nAgent\nUI-\nAGILE\nLeanabell-\nPro ver\nDeepSeek-\nPro ver-v2\nLeanabell-\nPro ver-v2\nSTP\nLeanPr ogress\nKim ina-\nPro ver\nInte rnLM 2. 5-\nSte pProve r\nLean-\nSTaR\nInFiGUI-R1\nAgentCPM\nUI-R1\nZeroGUI\nWebAgent-R1 UI-Venus\nDiGiRL\nUI-TARS\nMaAS\nMAPoRL\nMALT\nReMA\nLERO\nMARFT\nMMedAgent-RL MLPO\nFlow\nReasoner\nTime-R1\nG-Desig\n-ner\nGPT\nSwarm\nTimeMaster\nL-Zero\nAgent\nMod els\nARIA\nGiGPO\nSPA-RL\nTrinity-\nRFT\nSotopia\nRL\nAMPO\nSKyRL\nSQL\nWeak4\nStro ng\nAgent\nPrune\nAgent\nDr opou t\nSirius\nSirius\nACC-\nCollab\nThe Evolution Tree of RL for \nDomain-specific Agents\nMinimo\nPro ofN et++\nCh ain-of-\nA gen ts\nASearc her\nAtom-Sear cher\nMiro Mind\nOS-R1 Re:Form\nMSRL\nSeed-\nProver\nrStar2-\nAgen t\nComputerRL\nMAGRPO\nRL CC F\nFigure 6: The evolution tree of RL for domain-specific agents.\n4.1. Search & Research Agent\nSearch has been central to extending LLMs with external knowledge, with Retrieval-Augmented Generation\n(RAG) as a widely used approach [244, 245]. The paradigm is now evolving beyond simple information\n\nand Feng Zhao. Agent-FLAN: Designing data and methods of effective agent tuning for large\nlanguage models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors,Findings of the\nAssociation for Computational Linguistics: ACL 2024, pages 9354–9366, Bangkok, Thailand, August\n2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.557. URL\nhttps://aclanthology.org/2024.findings-acl.557/.\n[81] Yifan Song, Weimin Xiong, Xiutian Zhao, Dawei Zhu, Wenhao Wu, Ke Wang, Cheng Li, Wei Peng,\nand Sujian Li. Agentbank: Towards generalized llm agents via fine-tuning on 50000+ interaction\ntrajectories. InEMNLP (Findings), pages 2124–2141, 2024. URLhttps://aclanthology.org/\n2024.findings-emnlp.116.\n[82] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang,\nand Yongbin Li. Api-bank: A comprehensive benchmark for tool-augmented llms. InEMNLP, pages\n3102–3116, 2023. URLhttps://doi.org/10.18653/v1/2023.emnlp-main.187.\n\nKeywords: Agentic Reinforcement Learning, Large Language Models, LLM Agent\nDate: September 1st, 2025\nCode Repository: https://github.com/xhyumiracle/Awesome-AgenticLLM-RL-Papers\nCorresponding: jeremyyin@robots.ox.ac.uk, bailei@pjlab.org.cn\nMain Contact: guibinz@u.nus.edu, genghejia0530@gmail.com, x.yu21@imperial.ac.uk\n1\narXiv:2509.02547v1  [cs.AI]  2 Sep 2025 \nAnswer:<｜Assistant｜><think>\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=129648, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None.
INFO 09-10 23:26:21 [async_llm.py:269] Added request chatcmpl-56ffd8b332e54aa49b2c8233721c9d51.
INFO 09-10 23:26:23 [loggers.py:122] Engine 000: Avg prompt throughput: 142.4 tokens/s, Avg generation throughput: 39.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 70.6%
INFO:     127.0.0.1:60182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 09-10 23:26:33 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 70.6%
INFO 09-10 23:26:43 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 70.6%
INFO 09-10 23:28:12 [logger.py:41] Received request chatcmpl-335ede03e54f43daaef75d28d4a605a2: prompt: "<｜begin▁of▁sentence｜><｜User｜>You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: Instruct: Given a search query, retrieve relevant passages that answer the query\nQuery: Agent的定义是什么？ \nContext: Chen, Yankai Lin, Jie Xie, Wei Zhou, Wang Xu, Yuanheng Zhang, Zhou Su, Zhongwu Zhai, Xiaoming\nLiu, Yudong Mei, Jianming Xu, Hongyan Tian, Chongyi Wang, Chi Chen, Yuan Yao, Zhiyuan Liu, and\nMaosong Sun. Agentcpm-gui: Building mobile-use agents with reinforcement fine-tuning, 2025. URL\nhttps://arxiv.org/abs/2506.01391.\n82\n\nbenchmark for browsing agents, 2025. URLhttps://arxiv.org/abs/2504.12516.\n[267] Yihong Dong, Xue Jiang, Jiaru Qian, Tian Wang, Kechi Zhang, Zhi Jin, and Ge Li. A survey on code\ngeneration with llm-based agents, 2025. URLhttps://arxiv.org/abs/2508.00083.\n[268] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao\nZhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao,\nChenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for a multi-agent collaborative\nframework. InThe Twelfth International Conference on Learning Representations, 2024. URLhttps:\n//openreview.net/forum?id=VtmBAGCN7o.\n[269] Significant Gravitas. AutoGPT: Autonomous gpt-4 agent framework. GitHub, MIT License, 3 2023.\nURL https://github.com/Significant-Gravitas/AutoGPT. Initial release date.\n[270] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi\n\nInfiGUI\nAgent\nUI-\nAGILE\nLeanabell-\nPro ver\nDeepSeek-\nPro ver-v2\nLeanabell-\nPro ver-v2\nSTP\nLeanPr ogress\nKim ina-\nPro ver\nInte rnLM 2. 5-\nSte pProve r\nLean-\nSTaR\nInFiGUI-R1\nAgentCPM\nUI-R1\nZeroGUI\nWebAgent-R1 UI-Venus\nDiGiRL\nUI-TARS\nMaAS\nMAPoRL\nMALT\nReMA\nLERO\nMARFT\nMMedAgent-RL MLPO\nFlow\nReasoner\nTime-R1\nG-Desig\n-ner\nGPT\nSwarm\nTimeMaster\nL-Zero\nAgent\nMod els\nARIA\nGiGPO\nSPA-RL\nTrinity-\nRFT\nSotopia\nRL\nAMPO\nSKyRL\nSQL\nWeak4\nStro ng\nAgent\nPrune\nAgent\nDr opou t\nSirius\nSirius\nACC-\nCollab\nThe Evolution Tree of RL for \nDomain-specific Agents\nMinimo\nPro ofN et++\nCh ain-of-\nA gen ts\nASearc her\nAtom-Sear cher\nMiro Mind\nOS-R1 Re:Form\nMSRL\nSeed-\nProver\nrStar2-\nAgen t\nComputerRL\nMAGRPO\nRL CC F\nFigure 6: The evolution tree of RL for domain-specific agents.\n4.1. Search & Research Agent\nSearch has been central to extending LLMs with external knowledge, with Retrieval-Augmented Generation\n(RAG) as a widely used approach [244, 245]. The paradigm is now evolving beyond simple information\n\nand Feng Zhao. Agent-FLAN: Designing data and methods of effective agent tuning for large\nlanguage models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors,Findings of the\nAssociation for Computational Linguistics: ACL 2024, pages 9354–9366, Bangkok, Thailand, August\n2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.557. URL\nhttps://aclanthology.org/2024.findings-acl.557/.\n[81] Yifan Song, Weimin Xiong, Xiutian Zhao, Dawei Zhu, Wenhao Wu, Ke Wang, Cheng Li, Wei Peng,\nand Sujian Li. Agentbank: Towards generalized llm agents via fine-tuning on 50000+ interaction\ntrajectories. InEMNLP (Findings), pages 2124–2141, 2024. URLhttps://aclanthology.org/\n2024.findings-emnlp.116.\n[82] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang,\nand Yongbin Li. Api-bank: A comprehensive benchmark for tool-augmented llms. InEMNLP, pages\n3102–3116, 2023. URLhttps://doi.org/10.18653/v1/2023.emnlp-main.187.\n\nKeywords: Agentic Reinforcement Learning, Large Language Models, LLM Agent\nDate: September 1st, 2025\nCode Repository: https://github.com/xhyumiracle/Awesome-AgenticLLM-RL-Papers\nCorresponding: jeremyyin@robots.ox.ac.uk, bailei@pjlab.org.cn\nMain Contact: guibinz@u.nus.edu, genghejia0530@gmail.com, x.yu21@imperial.ac.uk\n1\narXiv:2509.02547v1  [cs.AI]  2 Sep 2025 \nAnswer:<｜Assistant｜><think>\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=129648, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None.
INFO 09-10 23:28:12 [async_llm.py:269] Added request chatcmpl-335ede03e54f43daaef75d28d4a605a2.
INFO 09-10 23:28:13 [loggers.py:122] Engine 000: Avg prompt throughput: 142.4 tokens/s, Avg generation throughput: 25.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 74.8%
INFO:     127.0.0.1:60608 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 09-10 23:28:23 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 74.8%
INFO 09-10 23:28:33 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 74.8%
INFO 09-10 23:33:44 [logger.py:41] Received request chatcmpl-4dc6792cff9842148d3b006b8e542a5e: prompt: "<｜begin▁of▁sentence｜><｜User｜>You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: Instruct: Given a search query, retrieve relevant passages that answer the query\nQuery: Agent的定义是什么？ \nContext: Chen, Yankai Lin, Jie Xie, Wei Zhou, Wang Xu, Yuanheng Zhang, Zhou Su, Zhongwu Zhai, Xiaoming\nLiu, Yudong Mei, Jianming Xu, Hongyan Tian, Chongyi Wang, Chi Chen, Yuan Yao, Zhiyuan Liu, and\nMaosong Sun. Agentcpm-gui: Building mobile-use agents with reinforcement fine-tuning, 2025. URL\nhttps://arxiv.org/abs/2506.01391.\n82\n\nbenchmark for browsing agents, 2025. URLhttps://arxiv.org/abs/2504.12516.\n[267] Yihong Dong, Xue Jiang, Jiaru Qian, Tian Wang, Kechi Zhang, Zhi Jin, and Ge Li. A survey on code\ngeneration with llm-based agents, 2025. URLhttps://arxiv.org/abs/2508.00083.\n[268] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao\nZhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao,\nChenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for a multi-agent collaborative\nframework. InThe Twelfth International Conference on Learning Representations, 2024. URLhttps:\n//openreview.net/forum?id=VtmBAGCN7o.\n[269] Significant Gravitas. AutoGPT: Autonomous gpt-4 agent framework. GitHub, MIT License, 3 2023.\nURL https://github.com/Significant-Gravitas/AutoGPT. Initial release date.\n[270] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi\n\nInfiGUI\nAgent\nUI-\nAGILE\nLeanabell-\nPro ver\nDeepSeek-\nPro ver-v2\nLeanabell-\nPro ver-v2\nSTP\nLeanPr ogress\nKim ina-\nPro ver\nInte rnLM 2. 5-\nSte pProve r\nLean-\nSTaR\nInFiGUI-R1\nAgentCPM\nUI-R1\nZeroGUI\nWebAgent-R1 UI-Venus\nDiGiRL\nUI-TARS\nMaAS\nMAPoRL\nMALT\nReMA\nLERO\nMARFT\nMMedAgent-RL MLPO\nFlow\nReasoner\nTime-R1\nG-Desig\n-ner\nGPT\nSwarm\nTimeMaster\nL-Zero\nAgent\nMod els\nARIA\nGiGPO\nSPA-RL\nTrinity-\nRFT\nSotopia\nRL\nAMPO\nSKyRL\nSQL\nWeak4\nStro ng\nAgent\nPrune\nAgent\nDr opou t\nSirius\nSirius\nACC-\nCollab\nThe Evolution Tree of RL for \nDomain-specific Agents\nMinimo\nPro ofN et++\nCh ain-of-\nA gen ts\nASearc her\nAtom-Sear cher\nMiro Mind\nOS-R1 Re:Form\nMSRL\nSeed-\nProver\nrStar2-\nAgen t\nComputerRL\nMAGRPO\nRL CC F\nFigure 6: The evolution tree of RL for domain-specific agents.\n4.1. Search & Research Agent\nSearch has been central to extending LLMs with external knowledge, with Retrieval-Augmented Generation\n(RAG) as a widely used approach [244, 245]. The paradigm is now evolving beyond simple information\n\nand Feng Zhao. Agent-FLAN: Designing data and methods of effective agent tuning for large\nlanguage models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors,Findings of the\nAssociation for Computational Linguistics: ACL 2024, pages 9354–9366, Bangkok, Thailand, August\n2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.557. URL\nhttps://aclanthology.org/2024.findings-acl.557/.\n[81] Yifan Song, Weimin Xiong, Xiutian Zhao, Dawei Zhu, Wenhao Wu, Ke Wang, Cheng Li, Wei Peng,\nand Sujian Li. Agentbank: Towards generalized llm agents via fine-tuning on 50000+ interaction\ntrajectories. InEMNLP (Findings), pages 2124–2141, 2024. URLhttps://aclanthology.org/\n2024.findings-emnlp.116.\n[82] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang,\nand Yongbin Li. Api-bank: A comprehensive benchmark for tool-augmented llms. InEMNLP, pages\n3102–3116, 2023. URLhttps://doi.org/10.18653/v1/2023.emnlp-main.187.\n\nKeywords: Agentic Reinforcement Learning, Large Language Models, LLM Agent\nDate: September 1st, 2025\nCode Repository: https://github.com/xhyumiracle/Awesome-AgenticLLM-RL-Papers\nCorresponding: jeremyyin@robots.ox.ac.uk, bailei@pjlab.org.cn\nMain Contact: guibinz@u.nus.edu, genghejia0530@gmail.com, x.yu21@imperial.ac.uk\n1\narXiv:2509.02547v1  [cs.AI]  2 Sep 2025 \nAnswer:<｜Assistant｜><think>\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=129648, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None.
INFO 09-10 23:33:44 [async_llm.py:269] Added request chatcmpl-4dc6792cff9842148d3b006b8e542a5e.
INFO:     127.0.0.1:36288 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 09-10 23:33:53 [loggers.py:122] Engine 000: Avg prompt throughput: 142.4 tokens/s, Avg generation throughput: 49.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 78.0%
INFO 09-10 23:34:03 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 78.0%
INFO 09-10 23:35:30 [logger.py:41] Received request chatcmpl-e8b28698c5d941c8b51ee683edee7767: prompt: "<｜begin▁of▁sentence｜><｜User｜>You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: Instruct: Given a search query, retrieve relevant passages that answer the query\nQuery: Agent的定义是什么？ \nContext: Chen, Yankai Lin, Jie Xie, Wei Zhou, Wang Xu, Yuanheng Zhang, Zhou Su, Zhongwu Zhai, Xiaoming\nLiu, Yudong Mei, Jianming Xu, Hongyan Tian, Chongyi Wang, Chi Chen, Yuan Yao, Zhiyuan Liu, and\nMaosong Sun. Agentcpm-gui: Building mobile-use agents with reinforcement fine-tuning, 2025. URL\nhttps://arxiv.org/abs/2506.01391.\n82\n\nbenchmark for browsing agents, 2025. URLhttps://arxiv.org/abs/2504.12516.\n[267] Yihong Dong, Xue Jiang, Jiaru Qian, Tian Wang, Kechi Zhang, Zhi Jin, and Ge Li. A survey on code\ngeneration with llm-based agents, 2025. URLhttps://arxiv.org/abs/2508.00083.\n[268] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao\nZhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao,\nChenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for a multi-agent collaborative\nframework. InThe Twelfth International Conference on Learning Representations, 2024. URLhttps:\n//openreview.net/forum?id=VtmBAGCN7o.\n[269] Significant Gravitas. AutoGPT: Autonomous gpt-4 agent framework. GitHub, MIT License, 3 2023.\nURL https://github.com/Significant-Gravitas/AutoGPT. Initial release date.\n[270] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi\n\nInfiGUI\nAgent\nUI-\nAGILE\nLeanabell-\nPro ver\nDeepSeek-\nPro ver-v2\nLeanabell-\nPro ver-v2\nSTP\nLeanPr ogress\nKim ina-\nPro ver\nInte rnLM 2. 5-\nSte pProve r\nLean-\nSTaR\nInFiGUI-R1\nAgentCPM\nUI-R1\nZeroGUI\nWebAgent-R1 UI-Venus\nDiGiRL\nUI-TARS\nMaAS\nMAPoRL\nMALT\nReMA\nLERO\nMARFT\nMMedAgent-RL MLPO\nFlow\nReasoner\nTime-R1\nG-Desig\n-ner\nGPT\nSwarm\nTimeMaster\nL-Zero\nAgent\nMod els\nARIA\nGiGPO\nSPA-RL\nTrinity-\nRFT\nSotopia\nRL\nAMPO\nSKyRL\nSQL\nWeak4\nStro ng\nAgent\nPrune\nAgent\nDr opou t\nSirius\nSirius\nACC-\nCollab\nThe Evolution Tree of RL for \nDomain-specific Agents\nMinimo\nPro ofN et++\nCh ain-of-\nA gen ts\nASearc her\nAtom-Sear cher\nMiro Mind\nOS-R1 Re:Form\nMSRL\nSeed-\nProver\nrStar2-\nAgen t\nComputerRL\nMAGRPO\nRL CC F\nFigure 6: The evolution tree of RL for domain-specific agents.\n4.1. Search & Research Agent\nSearch has been central to extending LLMs with external knowledge, with Retrieval-Augmented Generation\n(RAG) as a widely used approach [244, 245]. The paradigm is now evolving beyond simple information\n\nand Feng Zhao. Agent-FLAN: Designing data and methods of effective agent tuning for large\nlanguage models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors,Findings of the\nAssociation for Computational Linguistics: ACL 2024, pages 9354–9366, Bangkok, Thailand, August\n2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.557. URL\nhttps://aclanthology.org/2024.findings-acl.557/.\n[81] Yifan Song, Weimin Xiong, Xiutian Zhao, Dawei Zhu, Wenhao Wu, Ke Wang, Cheng Li, Wei Peng,\nand Sujian Li. Agentbank: Towards generalized llm agents via fine-tuning on 50000+ interaction\ntrajectories. InEMNLP (Findings), pages 2124–2141, 2024. URLhttps://aclanthology.org/\n2024.findings-emnlp.116.\n[82] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang,\nand Yongbin Li. Api-bank: A comprehensive benchmark for tool-augmented llms. InEMNLP, pages\n3102–3116, 2023. URLhttps://doi.org/10.18653/v1/2023.emnlp-main.187.\n\nKeywords: Agentic Reinforcement Learning, Large Language Models, LLM Agent\nDate: September 1st, 2025\nCode Repository: https://github.com/xhyumiracle/Awesome-AgenticLLM-RL-Papers\nCorresponding: jeremyyin@robots.ox.ac.uk, bailei@pjlab.org.cn\nMain Contact: guibinz@u.nus.edu, genghejia0530@gmail.com, x.yu21@imperial.ac.uk\n1\narXiv:2509.02547v1  [cs.AI]  2 Sep 2025 \nAnswer:<｜Assistant｜><think>\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=129648, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None.
INFO 09-10 23:35:30 [async_llm.py:269] Added request chatcmpl-e8b28698c5d941c8b51ee683edee7767.
INFO:     127.0.0.1:53954 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 09-10 23:35:33 [loggers.py:122] Engine 000: Avg prompt throughput: 142.4 tokens/s, Avg generation throughput: 32.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 80.4%
INFO 09-10 23:35:43 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 80.4%
INFO 09-10 23:37:17 [logger.py:41] Received request chatcmpl-9f6949a9e53b436fb783ff14c670ffe4: prompt: "<｜begin▁of▁sentence｜><｜User｜>You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: Instruct: Given a search query, retrieve relevant passages that answer the query\nQuery: Agent的定义是什么？ \nContext: Chen, Yankai Lin, Jie Xie, Wei Zhou, Wang Xu, Yuanheng Zhang, Zhou Su, Zhongwu Zhai, Xiaoming\nLiu, Yudong Mei, Jianming Xu, Hongyan Tian, Chongyi Wang, Chi Chen, Yuan Yao, Zhiyuan Liu, and\nMaosong Sun. Agentcpm-gui: Building mobile-use agents with reinforcement fine-tuning, 2025. URL\nhttps://arxiv.org/abs/2506.01391.\n82\n\nbenchmark for browsing agents, 2025. URLhttps://arxiv.org/abs/2504.12516.\n[267] Yihong Dong, Xue Jiang, Jiaru Qian, Tian Wang, Kechi Zhang, Zhi Jin, and Ge Li. A survey on code\ngeneration with llm-based agents, 2025. URLhttps://arxiv.org/abs/2508.00083.\n[268] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao\nZhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao,\nChenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for a multi-agent collaborative\nframework. InThe Twelfth International Conference on Learning Representations, 2024. URLhttps:\n//openreview.net/forum?id=VtmBAGCN7o.\n[269] Significant Gravitas. AutoGPT: Autonomous gpt-4 agent framework. GitHub, MIT License, 3 2023.\nURL https://github.com/Significant-Gravitas/AutoGPT. Initial release date.\n[270] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi\n\nInfiGUI\nAgent\nUI-\nAGILE\nLeanabell-\nPro ver\nDeepSeek-\nPro ver-v2\nLeanabell-\nPro ver-v2\nSTP\nLeanPr ogress\nKim ina-\nPro ver\nInte rnLM 2. 5-\nSte pProve r\nLean-\nSTaR\nInFiGUI-R1\nAgentCPM\nUI-R1\nZeroGUI\nWebAgent-R1 UI-Venus\nDiGiRL\nUI-TARS\nMaAS\nMAPoRL\nMALT\nReMA\nLERO\nMARFT\nMMedAgent-RL MLPO\nFlow\nReasoner\nTime-R1\nG-Desig\n-ner\nGPT\nSwarm\nTimeMaster\nL-Zero\nAgent\nMod els\nARIA\nGiGPO\nSPA-RL\nTrinity-\nRFT\nSotopia\nRL\nAMPO\nSKyRL\nSQL\nWeak4\nStro ng\nAgent\nPrune\nAgent\nDr opou t\nSirius\nSirius\nACC-\nCollab\nThe Evolution Tree of RL for \nDomain-specific Agents\nMinimo\nPro ofN et++\nCh ain-of-\nA gen ts\nASearc her\nAtom-Sear cher\nMiro Mind\nOS-R1 Re:Form\nMSRL\nSeed-\nProver\nrStar2-\nAgen t\nComputerRL\nMAGRPO\nRL CC F\nFigure 6: The evolution tree of RL for domain-specific agents.\n4.1. Search & Research Agent\nSearch has been central to extending LLMs with external knowledge, with Retrieval-Augmented Generation\n(RAG) as a widely used approach [244, 245]. The paradigm is now evolving beyond simple information\n\nand Feng Zhao. Agent-FLAN: Designing data and methods of effective agent tuning for large\nlanguage models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors,Findings of the\nAssociation for Computational Linguistics: ACL 2024, pages 9354–9366, Bangkok, Thailand, August\n2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.557. URL\nhttps://aclanthology.org/2024.findings-acl.557/.\n[81] Yifan Song, Weimin Xiong, Xiutian Zhao, Dawei Zhu, Wenhao Wu, Ke Wang, Cheng Li, Wei Peng,\nand Sujian Li. Agentbank: Towards generalized llm agents via fine-tuning on 50000+ interaction\ntrajectories. InEMNLP (Findings), pages 2124–2141, 2024. URLhttps://aclanthology.org/\n2024.findings-emnlp.116.\n[82] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang,\nand Yongbin Li. Api-bank: A comprehensive benchmark for tool-augmented llms. InEMNLP, pages\n3102–3116, 2023. URLhttps://doi.org/10.18653/v1/2023.emnlp-main.187.\n\nKeywords: Agentic Reinforcement Learning, Large Language Models, LLM Agent\nDate: September 1st, 2025\nCode Repository: https://github.com/xhyumiracle/Awesome-AgenticLLM-RL-Papers\nCorresponding: jeremyyin@robots.ox.ac.uk, bailei@pjlab.org.cn\nMain Contact: guibinz@u.nus.edu, genghejia0530@gmail.com, x.yu21@imperial.ac.uk\n1\narXiv:2509.02547v1  [cs.AI]  2 Sep 2025 \nAnswer:<｜Assistant｜><think>\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=129648, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None.
INFO 09-10 23:37:17 [async_llm.py:269] Added request chatcmpl-9f6949a9e53b436fb783ff14c670ffe4.
INFO:     127.0.0.1:52128 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 09-10 23:37:23 [loggers.py:122] Engine 000: Avg prompt throughput: 142.4 tokens/s, Avg generation throughput: 39.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 82.3%
INFO 09-10 23:37:33 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 82.3%
INFO 09-10 23:42:36 [logger.py:41] Received request chatcmpl-e4d219a60e144841b0b00f859e4f9a2a: prompt: "<｜begin▁of▁sentence｜><｜User｜>You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: Instruct: Given a search query, retrieve relevant passages that answer the query\nQuery: Agent的定义是什么？ \nContext: Chen, Yankai Lin, Jie Xie, Wei Zhou, Wang Xu, Yuanheng Zhang, Zhou Su, Zhongwu Zhai, Xiaoming\nLiu, Yudong Mei, Jianming Xu, Hongyan Tian, Chongyi Wang, Chi Chen, Yuan Yao, Zhiyuan Liu, and\nMaosong Sun. Agentcpm-gui: Building mobile-use agents with reinforcement fine-tuning, 2025. URL\nhttps://arxiv.org/abs/2506.01391.\n82\n\nbenchmark for browsing agents, 2025. URLhttps://arxiv.org/abs/2504.12516.\n[267] Yihong Dong, Xue Jiang, Jiaru Qian, Tian Wang, Kechi Zhang, Zhi Jin, and Ge Li. A survey on code\ngeneration with llm-based agents, 2025. URLhttps://arxiv.org/abs/2508.00083.\n[268] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao\nZhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao,\nChenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for a multi-agent collaborative\nframework. InThe Twelfth International Conference on Learning Representations, 2024. URLhttps:\n//openreview.net/forum?id=VtmBAGCN7o.\n[269] Significant Gravitas. AutoGPT: Autonomous gpt-4 agent framework. GitHub, MIT License, 3 2023.\nURL https://github.com/Significant-Gravitas/AutoGPT. Initial release date.\n[270] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi\n\nInfiGUI\nAgent\nUI-\nAGILE\nLeanabell-\nPro ver\nDeepSeek-\nPro ver-v2\nLeanabell-\nPro ver-v2\nSTP\nLeanPr ogress\nKim ina-\nPro ver\nInte rnLM 2. 5-\nSte pProve r\nLean-\nSTaR\nInFiGUI-R1\nAgentCPM\nUI-R1\nZeroGUI\nWebAgent-R1 UI-Venus\nDiGiRL\nUI-TARS\nMaAS\nMAPoRL\nMALT\nReMA\nLERO\nMARFT\nMMedAgent-RL MLPO\nFlow\nReasoner\nTime-R1\nG-Desig\n-ner\nGPT\nSwarm\nTimeMaster\nL-Zero\nAgent\nMod els\nARIA\nGiGPO\nSPA-RL\nTrinity-\nRFT\nSotopia\nRL\nAMPO\nSKyRL\nSQL\nWeak4\nStro ng\nAgent\nPrune\nAgent\nDr opou t\nSirius\nSirius\nACC-\nCollab\nThe Evolution Tree of RL for \nDomain-specific Agents\nMinimo\nPro ofN et++\nCh ain-of-\nA gen ts\nASearc her\nAtom-Sear cher\nMiro Mind\nOS-R1 Re:Form\nMSRL\nSeed-\nProver\nrStar2-\nAgen t\nComputerRL\nMAGRPO\nRL CC F\nFigure 6: The evolution tree of RL for domain-specific agents.\n4.1. Search & Research Agent\nSearch has been central to extending LLMs with external knowledge, with Retrieval-Augmented Generation\n(RAG) as a widely used approach [244, 245]. The paradigm is now evolving beyond simple information\n\nand Feng Zhao. Agent-FLAN: Designing data and methods of effective agent tuning for large\nlanguage models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors,Findings of the\nAssociation for Computational Linguistics: ACL 2024, pages 9354–9366, Bangkok, Thailand, August\n2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.557. URL\nhttps://aclanthology.org/2024.findings-acl.557/.\n[81] Yifan Song, Weimin Xiong, Xiutian Zhao, Dawei Zhu, Wenhao Wu, Ke Wang, Cheng Li, Wei Peng,\nand Sujian Li. Agentbank: Towards generalized llm agents via fine-tuning on 50000+ interaction\ntrajectories. InEMNLP (Findings), pages 2124–2141, 2024. URLhttps://aclanthology.org/\n2024.findings-emnlp.116.\n[82] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang,\nand Yongbin Li. Api-bank: A comprehensive benchmark for tool-augmented llms. InEMNLP, pages\n3102–3116, 2023. URLhttps://doi.org/10.18653/v1/2023.emnlp-main.187.\n\nKeywords: Agentic Reinforcement Learning, Large Language Models, LLM Agent\nDate: September 1st, 2025\nCode Repository: https://github.com/xhyumiracle/Awesome-AgenticLLM-RL-Papers\nCorresponding: jeremyyin@robots.ox.ac.uk, bailei@pjlab.org.cn\nMain Contact: guibinz@u.nus.edu, genghejia0530@gmail.com, x.yu21@imperial.ac.uk\n1\narXiv:2509.02547v1  [cs.AI]  2 Sep 2025 \nAnswer:<｜Assistant｜><think>\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=129648, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None.
INFO 09-10 23:42:36 [async_llm.py:269] Added request chatcmpl-e4d219a60e144841b0b00f859e4f9a2a.
INFO:     127.0.0.1:44718 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 09-10 23:42:43 [loggers.py:122] Engine 000: Avg prompt throughput: 142.4 tokens/s, Avg generation throughput: 37.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 83.9%
INFO 09-10 23:42:53 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 83.9%
INFO 09-11 20:51:38 [logger.py:41] Received request chatcmpl-16607144a3c94405b1a229cc03deeb33: prompt: "<｜begin▁of▁sentence｜><｜User｜>You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: Instruct: Given a search query, retrieve relevant passages that answer the query\nQuery: 设备出现故障该如何解决？ \nContext: 第三节  需注意的安全事项 ......................................................................................................................... 18 \n第四节  生产过程的控制 ............................................................................................................................. 19 \n第五节  生产结束后的工作 ......................................................................................................................... 21 \n第三章  设备的日常维护保养 ............................................................................................................................. 23 \n第一节  机械部分的日常维护保养 ............................................................................................................. 23 \n第二节  电气部分的日常维护保养 ............................................................................................................. 28 \n第四章  常见印刷问题的解决方法 ..................................................................................................................... 29\n\n故障 5：带纸压轮的压力不能调整。 .................................................................................................. 92 \n故障 6：模切系统故障。 ...................................................................................................................... 93 \n故障 7：模切单元各项参数数据与实际调整的数据不准的校正方法。 ........................................... 93 \n七、堆叠单元（标准型）常见故障的解决方法.......................................................................................... 94 \n故障 1：堆叠机不能上升。 .................................................................................................................. 94 \n故障 2：堆叠机不能下降。 .................................................................................................................. 94 \n故障 3：传动皮带不能转动。 .............................................................................................................. 95 \n八、主电脑触摸屏系统重新恢复的处理 ..................................................................................................... 95\n\n三、印刷单元常见故障的解决方法 ............................................................................................................. 75 \n故障 1：印刷单元没有电源。 .............................................................................................................. 75 \n故障 2：相位没有数据显示，在单元合拢复位的时候印刷滚筒不能停止运行。 ........................... 75 \n故障 3：相位不能调整，相位电机不运行。 ....................................................................................... 76 \n故障 4：相位没有数据显示，机台合拢后印刷滚筒不能停止运行。 ............................................... 76 \n故障 5：相位有数据显示，机台合拢后印刷滚筒不能停止运行。 ................................................... 77 \n故障 6：印刷轴向不能调整。 .............................................................................................................. 77 \n故障 7：压印间隙不能调整。 .............................................................................................................. 78 \n故障 8：网纹辊间隙不能调整。 .......................................................................................................... 79\n\nTOPRA GD型水性印刷机培训手册 \n第 94 页                                                \n据校正如图 7-78所示： \n \n \n \n \n \n                                                                               图 7-78 \n \n \n \n \n               \n七、堆叠单元（标准型）常见故障的解决方法 \n故障 1：堆叠机不能上升。 \n检查维修的方法具体如下： \n1） 、检查变频器是否过载，若出现过载情况，可按上图7-74 复位的方法复位即可； \n2） 、检查最高限位行程开关X3，次高位行程开关 X4 是否松动 (X3 和 X4 是 FX1N-PLC 输\n入的信号地址） ；如下图7-79 所示： \n3） 、检查操作面板上控制上升和下降的旋钮开关线是否接触良好（在电柜内PLC 的输入\n地址是 X11）。 \n \n \n \n \n \n \n \n \n \n图 7-79                                   图 7-80 \n故障 2：堆叠机不能下降。        \n检查维修的方法具体如下：\n\n第二节  GD机型各项电器原理结构介绍..................................................................................................... 51 \n第三节  GD机型整机“归零”调整 ............................................................................................................ 56 \n第四节  电气常见故障的解决方法 ............................................................................................................. 58 \n一、主控电脑部分常见故障的解决方法 ..................................................................................................... 58 \n故障 1：当主电柜开关为“ON”的时候，整机没有电源。 ............................................................... 58 \n故障 2：整机有控制电源，电脑有画面，但单元不能移动，主机不能启动。 ............................... 59 \n故障 3：主机不能启动，但主电脑控制台和所有单元有控制电源，而且网络通讯正常。 ........... 61 \n故障 4：系统供电正常，但单元机组无法移动，主机不能启动。 ................................................... 65 \n故障 5：系统供电正常，每个单元有220V 控制电压，也有380V 动力电源，而且安全开关电路也\n正常，但主机不能启动，车壁不能移动。.......................................................................................... 67 \nAnswer:<｜Assistant｜><think>\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=130039, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None.
INFO 09-11 20:51:38 [async_llm.py:269] Added request chatcmpl-16607144a3c94405b1a229cc03deeb33.
INFO:     127.0.0.1:56832 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 09-11 20:51:47 [loggers.py:122] Engine 000: Avg prompt throughput: 103.3 tokens/s, Avg generation throughput: 49.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 79.0%
INFO 09-11 20:51:57 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 79.0%
INFO 09-11 20:53:07 [logger.py:41] Received request chatcmpl-4399a68b237f4188be274235c5e34922: prompt: "<｜begin▁of▁sentence｜><｜User｜>You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: Instruct: Given a search query, retrieve relevant passages that answer the query\nQuery: 晨报记录怎么样 \nContext: 工作表: 环球印务\n\n列标题: 环球印务[002799.SZ] - 利润表 | 2021-03-31 | 2020-12-31 | 2019-12-31 | 2018-12-31 | 2017-12-31 | 2016-12-31 | 2015-12-31 | 2014-12-31 | 2013-12-31 | 2012-12-31 | 2011-12-31\n\n第1行: 报告期 | 一季报 | 年报 | 年报 | 年报 | 年报 | 年报 | 年报 | 年报 | 年报 | 年报 | 年报\n第2行: 报表类型 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表\n第3行: 营业总收入 | 16017.11 | 68146.26 | 68476.91 | 66545.37 | 59853.99 | 48195.55 | 50285.8 | 51457.13 | 42145.24 | 36707.37 | 33928.5\n第4行: \u3000\u3000营业收入 | 16017.11 | 68146.26 | 68476.91 | 66545.37 | 59853.99 | 48195.55 | 50285.8 | 51457.13 | 42145.24 | 36707.37 | 33928.5\n第5行: \u3000\u3000其他类金融业务收入 |  |  |  |  |  |  |  |  |  |  | \n第6行: 营业总成本 | 15574.66 | 63631.2 | 65402.05 | 63572.26 | 57265.41 | 45249.83 | 46444.36 | 44572.62 | 35879.3 | 30635.69 | 27876.41\n第7行: \u3000\u3000营业成本 | 12742.83 | 52248.99 | 53906.68 | 53970.47 | 48519.65 | 38406.74 | 39543.46 | 38682.92 | 30803.3 | 25963.36 | 23868.17\n第8行: \u3000\u3000税金及附加 | 119.26 | 576.67 | 446.7 | 573.33 | 413.93 | 342.48 | 284.2 | 228.08 | 216.39 | 230.2 | 243.25\n第9行: \u3000\u3000销售费用 | 577.13 | 2445.21 | 3154.98 | 2762.39 | 2656.02 | 2048.39 | 1955.2 | 1550.9 | 1298.34 | 1200.84 | 1059.33\n\n第1行: 报告期 | 一季报 | 年报 | 年报 | 年报 | 年报 | 年报 | 年报 | 年报 | 年报 | 年报 | 年报\n第2行: 报表类型 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表\n第3行: 营业总收入 | 81948.1 | 336526.21 | 339213.26 | 324894.55 | 285741.93 | 221927.64 | 201642.29 | 156322.23 | 130563.7 | 103350.06 | 84010.88\n第4行: \u3000\u3000营业收入 | 81948.1 | 336526.21 | 339213.26 | 324894.55 | 285741.93 | 221927.64 | 201642.29 | 156322.23 | 130563.7 | 103350.06 | 84010.88\n第5行: \u3000\u3000其他类金融业务收入 |  |  |  |  |  |  |  |  |  |  | \n第6行: 营业总成本 | 77826.69 | 319296.91 | 294625.03 | 284056.14 | 246654.75 | 202732.06 | 191688.77 | 126807.92 | 109975.92 | 90428.44 | 73379.47\n第7行: \u3000\u3000营业成本 | 62551.08 | 256679.37 | 230411.02 | 219621.01 | 187196.58 | 156568.96 | 148095.97 | 101780.62 | 87156.9 | 74695.24 | 57448.71\n第8行: \u3000\u3000税金及附加 | 852.29 | 2984.53 | 3438.81 | 3641.58 | 2890.04 | 1960.19 | 1158.24 | 1134.24 | 460.67 | 322.77 | 211.18\n\n第1行: 报告期 | 一季报 | 年报 | 年报 | 年报 | 年报 | 年报 | 年报 | 年报 | 年报 | 年报 | 年报\n第2行: 报表类型 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表\n第3行: 营业总收入 | 97121.47 | 306939.39 | 317292.23 | 332804.94 | 280234.71 | 234163.69 | 221944.21 | 200204.58 | 180152.67 | 176509.74 | 153522.02\n第4行: \u3000\u3000营业收入 | 97121.47 | 306939.39 | 317292.23 | 332804.94 | 280234.71 | 234163.69 | 221944.21 | 200204.58 | 180152.67 | 176509.74 | 153522.02\n第5行: \u3000\u3000其他类金融业务收入 |  |  |  |  |  |  |  |  |  |  | \n第6行: 营业总成本 | 79103.81 | 257217.62 | 250473.28 | 266922.77 | 213306.34 | 175410.93 | 146374.99 | 123862.6 | 109989.02 | 109115.94 | 94441.59\n第7行: \u3000\u3000营业成本 | 64241.92 | 193166.87 | 188385.42 | 204243.18 | 162433.09 | 129314.69 | 110824.54 | 93167.54 | 83632.96 | 84342.95 | 72356.48\n第8行: \u3000\u3000税金及附加 | 885.44 | 2926.66 | 3642.76 | 4003.23 | 3426.1 | 3211.6 | 2184.4 | 1925.54 | 1858.6 | 1822.85 | 1690.44\n\n第1行: 报告期 | 一季报 | 年报 | 年报 | 年报 | 年报 | 年报 | 年报 | 年报 | 年报 | 年报 | 年报\n第2行: 报表类型 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表 | 合并报表\n第3行: 营业总收入 | 25436.91 | 219329.3 | 109327.13 | 136150.38 | 133413.39 | 173309.22 | 154292.06 | 198474.13 | 191734.19 | 148917.49 | 116219.04\n第4行: \u3000\u3000营业收入 | 25436.91 | 219329.3 | 109327.13 | 136150.38 | 133413.39 | 173309.22 | 154292.06 | 198474.13 | 191734.19 | 148917.49 | 116219.04\n第5行: \u3000\u3000其他类金融业务收入 |  |  |  |  |  |  |  |  |  |  | \n第6行: 营业总成本 | 24716.06 | 212318.38 | 118228.49 | 138723.92 | 136680.29 | 174835.09 | 150178.56 | 195117.13 | 190522.42 | 147824.28 | 116270.94\n第7行: \u3000\u3000营业成本 | 19498.23 | 188947.43 | 88108.38 | 108658.25 | 111786.2 | 144953 | 121220.35 | 165383.57 | 159463.03 | 125174.84 | 95678.83\n第8行: \u3000\u3000税金及附加 | 153.53 | 2207.96 | 995.86 | 1346.54 | 2176.73 | 4654.31 | 4896.77 | 5718.54 | 7716.51 | 1787.91 | 1513.61 \nAnswer:<｜Assistant｜><think>\n", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=127678, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None.
INFO 09-11 20:53:07 [async_llm.py:269] Added request chatcmpl-4399a68b237f4188be274235c5e34922.
INFO:     127.0.0.1:60698 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 09-11 20:53:17 [loggers.py:122] Engine 000: Avg prompt throughput: 339.4 tokens/s, Avg generation throughput: 62.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 65.7%
INFO 09-11 20:53:27 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 65.7%
